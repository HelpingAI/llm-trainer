{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94263ea",
   "metadata": {},
   "source": [
    "# All Tokenizers Demo\n",
    "This notebook demonstrates all available tokenizer types in the `llm-trainer` framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_trainer.tokenizer import create_tokenizer\n",
    "import os\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. Hello world! 12345.\"\n",
    "print(f\"Original text: {sample_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815486f3",
   "metadata": {},
   "source": [
    "## 1. BPE Tokenizer (Byte Pair Encoding)\n",
    "BPE is the most common tokenizer for modern LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a small BPE tokenizer\n",
    "bpe_tokenizer = create_tokenizer(\"bpe\")\n",
    "bpe_tokenizer.train([sample_text], vocab_size=100)\n",
    "\n",
    "encoded = bpe_tokenizer.encode(sample_text)\n",
    "decoded = bpe_tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"BPE Encoded: {encoded}\")\n",
    "print(f\"BPE Decoded: {decoded}\")\n",
    "print(f\"BPE Tokens: {[bpe_tokenizer.decode([t]) for t in encoded]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001c4ae",
   "metadata": {},
   "source": [
    "## 2. WordPiece Tokenizer\n",
    "Commonly used in BERT models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer = create_tokenizer(\"wordpiece\")\n",
    "wp_tokenizer.train([sample_text], vocab_size=100)\n",
    "\n",
    "encoded = wp_tokenizer.encode(sample_text)\n",
    "print(f\"WordPiece Tokens: {[wp_tokenizer.decode([t]) for t in encoded]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98daaaec",
   "metadata": {},
   "source": [
    "## 3. SentencePiece Tokenizer\n",
    "Commonly used in Llama and T5 models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_tokenizer = create_tokenizer(\"sentencepiece\")\n",
    "sp_tokenizer.train([sample_text], vocab_size=100)\n",
    "\n",
    "encoded = sp_tokenizer.encode(sample_text)\n",
    "print(f\"SentencePiece Tokens: {[sp_tokenizer.decode([t]) for t in encoded]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7e431",
   "metadata": {},
   "source": [
    "## 4. Character and Byte-level BPE\n",
    "Character-level tokenization and GPT-2 style byte-level BPE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer = create_tokenizer(\"char\")\n",
    "char_tokenizer.train([sample_text])\n",
    "print(f\"Char Tokens: {[char_tokenizer.decode([t]) for t in char_tokenizer.encode(sample_text)]}\")\n",
    "\n",
    "byte_tokenizer = create_tokenizer(\"bytebpe\")\n",
    "byte_tokenizer.train([sample_text], vocab_size=100)\n",
    "print(f\"ByteBPE Tokens: {[byte_tokenizer.decode([t]) for t in byte_tokenizer.encode(sample_text)]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6432b5e",
   "metadata": {},
   "source": [
    "## 5. HuggingFace Pretrained Tokenizer\n",
    "Load any tokenizer from the HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4abc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "hf_tokenizer = create_tokenizer(\"hf\", pretrained_path=\"gpt2\")\n",
    "\n",
    "encoded = hf_tokenizer.encode(sample_text)\n",
    "print(f\"HF GPT-2 Tokens: {[hf_tokenizer.decode([t]) for t in encoded]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
