{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparing Different Tokenizers\n",
        "\n",
        "This notebook compares different tokenizer types to help you choose the right one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_trainer.tokenizer import create_tokenizer, get_available_tokenizers\n",
        "\n",
        "# See available tokenizers\n",
        "available = get_available_tokenizers()\n",
        "print(\"Available tokenizers:\")\n",
        "for tokenizer_type, description in available.items():\n",
        "    print(f\"  â€¢ {tokenizer_type}: {description}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test text\n",
        "test_text = \"The quick brown fox jumps over the lazy dog. Hello world!\"\n",
        "print(f\"Test text: {test_text}\")\n",
        "print(f\"Character count: {len(test_text)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare tokenizers\n",
        "tokenizer_types = [\"simple\", \"char\", \"bpe\"]\n",
        "results = {}\n",
        "\n",
        "for tokenizer_type in tokenizer_types:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Testing {tokenizer_type.upper()} tokenizer\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Create and train tokenizer\n",
        "    tokenizer = create_tokenizer(tokenizer_type)\n",
        "    \n",
        "    # Train on sample data\n",
        "    training_texts = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Hello world! This is a test.\",\n",
        "        \"Tokenization is important for NLP.\"\n",
        "    ] * 10  # Repeat for more data\n",
        "    \n",
        "    tokenizer.train(training_texts, vocab_size=500, verbose=False)\n",
        "    \n",
        "    # Test encoding\n",
        "    token_ids = tokenizer.encode(test_text, add_special_tokens=False)\n",
        "    decoded = tokenizer.decode(token_ids)\n",
        "    \n",
        "    results[tokenizer_type] = {\n",
        "        \"vocab_size\": tokenizer.vocab_size,\n",
        "        \"num_tokens\": len(token_ids),\n",
        "        \"token_ids\": token_ids[:10],  # First 10\n",
        "        \"decoded\": decoded\n",
        "    }\n",
        "    \n",
        "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "    print(f\"Number of tokens: {len(token_ids)}\")\n",
        "    print(f\"First 10 token IDs: {token_ids[:10]}\")\n",
        "    print(f\"Decoded text: {decoded}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Tokenizer':<15} {'Vocab Size':<15} {'Tokens':<10} {'Compression'}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for tokenizer_type, result in results.items():\n",
        "    compression = len(test_text) / result[\"num_tokens\"] if result[\"num_tokens\"] > 0 else 0\n",
        "    print(f\"{tokenizer_type:<15} {result['vocab_size']:<15} {result['num_tokens']:<10} {compression:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
