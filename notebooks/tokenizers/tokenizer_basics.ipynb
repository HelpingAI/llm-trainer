{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Basics\n",
    "\n",
    "This notebook demonstrates the basics of using tokenizers in LLM Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building simple word-level vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|██████████| 3/3 [00:00<00:00, 7752.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20 words\n",
      "Special tokens: 4\n",
      "Regular words: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llm_trainer.tokenizer import create_tokenizer\n",
    "\n",
    "# Create a simple tokenizer\n",
    "tokenizer = create_tokenizer(\"simple\")\n",
    "\n",
    "# Train on sample text\n",
    "texts = [\n",
    "    \"Hello world! This is a test.\",\n",
    "    \"Tokenization breaks text into tokens.\",\n",
    "    \"Each token gets a unique ID.\"\n",
    "]\n",
    "\n",
    "tokenizer.train(texts, vocab_size=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello world!\n",
      "Token IDs: [2, 5, 6, 3]\n",
      "Number of tokens: 4\n"
     ]
    }
   ],
   "source": [
    "# Encode text\n",
    "text = \"Hello world!\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Number of tokens: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Decode back to text\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20\n",
      "Sample tokens: ['<pad>', '<unk>', '<bos>', '<eos>', 'a', 'Hello', 'world!', 'This', 'is', 'test.']\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary information\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Sample tokens: {list(tokenizer.get_vocab().keys())[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
