{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizer Training\n",
    "\n",
    "This notebook shows how to train a BPE tokenizer from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_trainer.tokenizer import create_tokenizer\n",
    "\n",
    "# Create BPE tokenizer\n",
    "tokenizer = create_tokenizer(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1801350, using 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading texts:   0%|          | 1000/1801350 [00:00<01:42, 17549.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 647 texts\n",
      "Training BPE tokenizer...\n",
      "Collecting word frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 647/647 [00:00<00:00, 2239.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4170 unique words\n",
      "Initial vocabulary size: 120\n",
      "Learning 3080 BPE merges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning merges: 100%|██████████| 3080/3080 [01:01<00:00, 49.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocabulary size: 3200\n",
      "Learned 3080 merges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
    "text_column = 'text'\n",
    "\n",
    "tokenizer.train(\n",
    "    dataset=dataset,\n",
    "    vocab_size=3200,\n",
    "    max_samples=1000,\n",
    "    text_column=text_column,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The quick brown fox jumps over the lazy dog.\n",
      "Token IDs: [2, 185, 1, 330, 787, 1, 844, 2705, 1, 65, 74, 521, 1, 69, 256, 472, 1, 551, 1, 126]...\n",
      "Total tokens: 29\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "token_ids = tokenizer.encode(test_text)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Token IDs: {token_ids[:20]}...\")  # Show first 20\n",
    "print(f\"Total tokens: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved!\n"
     ]
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"./saved_tokenizer\")\n",
    "print(\"Tokenizer saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer vocab size: 3200\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "loaded_tokenizer = create_tokenizer(\"bpe\", pretrained_path=\"./saved_tokenizer\")\n",
    "print(f\"Loaded tokenizer vocab size: {loaded_tokenizer.vocab_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
