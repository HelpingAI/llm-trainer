{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26759021",
   "metadata": {},
   "source": [
    "# Text Classification Model Training\n",
    "This notebook demonstrates how to train a text classification model using the `llm-trainer` framework with mixed precision and tqdm progress tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from llm_trainer.config import ModelConfig, TrainingConfig, DataConfig\n",
    "from llm_trainer.models import TransformerLM\n",
    "from llm_trainer.tokenizer import create_tokenizer\n",
    "from llm_trainer.training import Trainer\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59263c30",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset\n",
    "We'll use a sample dataset for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small classification dataset (e.g., sentiment analysis)\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = create_tokenizer(\"bpe\")\n",
    "tokenizer.train(dataset, vocab_size=5000, text_column=\"text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f09764",
   "metadata": {},
   "source": [
    "## 2. Configure Model and Training with Mixed Precision\n",
    "We'll enable `fp16` or `bf16` for efficient training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4\n",
    ")\n",
    "\n",
    "# Configure training with mixed precision (bf16 if supported, else fp16)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_epochs=3,\n",
    "    fp16=not use_bf16,\n",
    "    bf16=use_bf16,\n",
    "    logging_steps=10,\n",
    "    checkpoint_dir=\"./checkpoints/classification\"\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerLM(model_config)\n",
    "print(f\"Model initialized with {model.get_num_params():,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726894b",
   "metadata": {},
   "source": [
    "## 3. Initialize Trainer and Run Training with tqdm\n",
    "The trainer will use `tqdm` for progress tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "# Configure data\n",
    "data_config = DataConfig(\n",
    "    dataset_name=\"imdb\",\n",
    "    max_length=512,\n",
    "    text_column=\"text\"\n",
    ")\n",
    "\n",
    "# Start training (this will show tqdm progress bars)\n",
    "trainer.train_from_config(model_config, data_config)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
