{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Generation with Trained Model\n",
        "\n",
        "This notebook shows how to generate text using a trained language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from llm_trainer.models import TransformerLM\n",
        "from llm_trainer.tokenizer import create_tokenizer\n",
        "from llm_trainer.utils.generation import TextGenerator, GenerationConfig\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = TransformerLM.from_pretrained(\"./trained_model\")\n",
        "tokenizer = create_tokenizer(\"bpe\", pretrained_path=\"./trained_model/tokenizer\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create generator\n",
        "generator = TextGenerator(model, tokenizer)\n",
        "\n",
        "# Configure generation\n",
        "gen_config = GenerationConfig(\n",
        "    max_length=100,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text\n",
        "prompt = \"The future of artificial intelligence\"\n",
        "generated = generator.generate(prompt, gen_config)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated: {generated[0]}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
